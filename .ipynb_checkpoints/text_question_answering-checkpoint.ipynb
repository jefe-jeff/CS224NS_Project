{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "speaking-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mount point: /home/jupyter/data\n",
      "2021/03/14 01:36:54.707940 Opening GCS connection...\n",
      "2021/03/14 01:36:54.905357 Mounting file system...\n",
      "2021/03/14 01:36:54.934078 File system has been successfully mounted.\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/gcsfuse spoken-squad-data data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspected-eugene",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.3.3 in /opt/conda/lib/python3.7/site-packages (4.3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (2020.11.13)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (3.7.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (0.0.43)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (20.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (0.10.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.3) (4.58.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.3.3) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.3.3) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.3.3) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.3) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.3) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.3) (1.26.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.3) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.3) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.3) (1.15.0)\n",
      "Requirement already satisfied: torchaudio==0.8.0 in /opt/conda/lib/python3.7/site-packages (0.8.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /opt/conda/lib/python3.7/site-packages (from torchaudio==0.8.0) (1.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0->torchaudio==0.8.0) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0->torchaudio==0.8.0) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_PATH = 'data'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "  %mkdir $DATA_PATH\n",
    "\n",
    "\n",
    "MODEL_PATH = 'trained_models'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "  %mkdir $MODEL_PATH\n",
    "\n",
    "if not os.path.exists('utils.py'):\n",
    "  !wget -q http://web.stanford.edu/class/cs224s/download/utils.py\n",
    "\n",
    "!pip3 -q install pytorch_lightning\n",
    "!pip3 install wandb -qqq\n",
    "#!pip3 install pytorch-pretrained-bert pytorch-nlp pytorch_transformers\n",
    "#!pip3 install pytorch_transformers\n",
    "!pip3 install transformers==4.3.3\n",
    "!pip3 install torchaudio==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collaborative-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2Model\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, RobertaForQuestionAnswering\n",
    "\n",
    "from squad_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "returning-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dirty-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_squad('data/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('data/dev-v2.0.json')\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bibliographic-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-deputy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 37257,   501,  ...,     1,     1,     1],\n",
      "        [    0,  3762,  6680,  ...,     1,     1,     1],\n",
      "        [    0, 23754,  4484,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   713, 11577,  ...,     1,     1,     1],\n",
      "        [    0,  4993,     5,  ...,     1,     1,     1],\n",
      "        [    0,   673, 32027,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 55, 170,  60,  20,  58,  55,  49,  16,   6,  41, 253,  65,  32,  63,\n",
      "         22,  12]), 'end_positions': tensor([ 56, 176,  61,  22,  61,  57,  51,  18,   7,  42, 259,  70,  36,  73,\n",
      "         27,  24])}\n"
     ]
    }
   ],
   "source": [
    "for t, batch in enumerate(val_loader):\n",
    "    print(batch)\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    loss, outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "egyptian-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify.\n",
    "\n",
    "class LightningTextBERTQA(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning class for training a BERT-QA model.\"\"\"\n",
    "    def __init__(self,learning_rate=1e-5, batch_size=16, weight_decay=1e-5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay   \n",
    "        self.train_dataset, self.val_dataset= \\\n",
    "          self.create_datasets()\n",
    "\n",
    "        self.model = self.create_model()\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        self.output=[]\n",
    "\n",
    "    def create_model(self):\n",
    "        model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = False   \n",
    "            \n",
    "        return model\n",
    "\n",
    "    def create_datasets(self):\n",
    "        \n",
    "        train_contexts, train_questions, train_answers = read_squad('data/train-v2.0.json')\n",
    "        val_contexts, val_questions, val_answers = read_squad('data/dev-v2.0.json')\n",
    "\n",
    "        add_end_idx(train_answers, train_contexts)\n",
    "        add_end_idx(val_answers, val_contexts)\n",
    "\n",
    "        \n",
    "        train_encodings = self.tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "        val_encodings = self.tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "\n",
    "\n",
    "        add_token_positions(train_encodings, train_answers)\n",
    "        add_token_positions(val_encodings, val_answers)\n",
    "\n",
    "        train_dataset = SquadDataset(train_encodings)\n",
    "        val_dataset = SquadDataset(val_encodings)\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(self.model.parameters(),\n",
    "                                  lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return [optim], [] # <-- put scheduler in here if you want to use one\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, start_positions, end_positions):\n",
    "        self.output = self.model(input_ids, attention_masks=attention_masks, start_positions=start_positions, end_positions=end_positions)\n",
    "        return self.output.loss, self.output.start_logits, self.output.end_logits, self.output.hidden_states[0]\n",
    "\n",
    "    def get_primary_task_loss(self, batch):\n",
    "        \"\"\"Returns ASR model losses, metrics, and embeddings for a batch.\"\"\"\n",
    "        input_ids, attention_masks = batch[0], batch[1]\n",
    "        start_positions, end_positions = batch[2], batch[3]\n",
    "\n",
    "        \n",
    "        loss, start_logits,end_logits,_  = self.forward(\n",
    "              input_ids, attention_masks, start_positions, end_positions)\n",
    "\n",
    "        return loss, start_logits,end_logits,embedding\n",
    "\n",
    "      # Overwrite TRAIN\n",
    "    def training_step(self, batch):\n",
    "        loss,_,_,_ = self.get_primary_task_loss(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "      # Overwrite VALIDATION: get next minibatch\n",
    "    def validation_step(self, batch):\n",
    "        loss,_,_,_ = self.get_primary_task_loss(batch)\n",
    "        return metrics\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # - important to shuffle to not overfit!\n",
    "        # - drop the last batch to preserve consistent batch sizes\n",
    "        loader = DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                            shuffle=True, pin_memory=True, drop_last=True)\n",
    "        return loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        loader = DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                            shuffle=False, pin_memory=True)\n",
    "        return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "vanilla-brush",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '0_0.flac',\n",
       " '0_0_0.flac',\n",
       " '0_0_1.flac',\n",
       " '0_0_2.flac',\n",
       " '0_0_3.flac',\n",
       " '0_0_4.flac',\n",
       " '0_0_5.flac',\n",
       " '0_0_6.flac',\n",
       " '0_0_7.flac',\n",
       " '0_0_8.flac',\n",
       " '0_1.flac',\n",
       " '0_10.flac',\n",
       " '0_10_0.flac',\n",
       " '0_10_1.flac',\n",
       " '0_10_2.flac',\n",
       " '0_10_3.flac',\n",
       " '0_12.flac',\n",
       " '0_12_0.flac',\n",
       " '0_12_1.flac',\n",
       " '0_12_2.flac',\n",
       " '0_12_3.flac',\n",
       " '0_12_4.flac',\n",
       " '0_12_5.flac',\n",
       " '0_13.flac',\n",
       " '0_13_0.flac',\n",
       " '0_13_1.flac',\n",
       " '0_13_2.flac',\n",
       " '0_13_3.flac',\n",
       " '0_13_4.flac',\n",
       " '0_13_5.flac',\n",
       " '0_14.flac',\n",
       " '0_14_0.flac',\n",
       " '0_14_1.flac',\n",
       " '0_14_2.flac',\n",
       " '0_14_3.flac',\n",
       " '0_14_4.flac',\n",
       " '0_14_5.flac',\n",
       " '0_14_6.flac',\n",
       " '0_15.flac',\n",
       " '0_15_0.flac',\n",
       " '0_15_1.flac',\n",
       " '0_15_2.flac',\n",
       " '0_15_3.flac',\n",
       " '0_15_4.flac',\n",
       " '0_15_5.flac',\n",
       " '0_16.flac',\n",
       " '0_16_0.flac',\n",
       " '0_16_1.flac',\n",
       " '0_16_2.flac',\n",
       " '0_16_3.flac',\n",
       " '0_16_4.flac',\n",
       " '0_16_5.flac',\n",
       " '0_16_6.flac',\n",
       " '0_16_7.flac',\n",
       " '0_17.flac',\n",
       " '0_17_0.flac',\n",
       " '0_17_1.flac',\n",
       " '0_17_2.flac',\n",
       " '0_17_3.flac',\n",
       " '0_18.flac',\n",
       " '0_18_0.flac',\n",
       " '0_18_1.flac',\n",
       " '0_18_2.flac',\n",
       " '0_18_3.flac',\n",
       " '0_18_4.flac',\n",
       " '0_18_5.flac',\n",
       " '0_19.flac',\n",
       " '0_19_0.flac',\n",
       " '0_19_1.flac',\n",
       " '0_19_2.flac',\n",
       " '0_19_3.flac',\n",
       " '0_19_4.flac',\n",
       " '0_19_5.flac',\n",
       " '0_19_6.flac',\n",
       " '0_1_0.flac',\n",
       " '0_1_1.flac',\n",
       " '0_1_2.flac',\n",
       " '0_1_3.flac',\n",
       " '0_1_4.flac',\n",
       " '0_1_5.flac',\n",
       " '0_1_6.flac',\n",
       " '0_1_7.flac',\n",
       " '0_2.flac',\n",
       " '0_20.flac',\n",
       " '0_20_0.flac',\n",
       " '0_20_1.flac',\n",
       " '0_20_2.flac',\n",
       " '0_20_3.flac',\n",
       " '0_21.flac',\n",
       " '0_21_0.flac',\n",
       " '0_21_1.flac',\n",
       " '0_21_2.flac',\n",
       " '0_21_3.flac',\n",
       " '0_21_4.flac',\n",
       " '0_22.flac',\n",
       " '0_22_0.flac',\n",
       " '0_22_1.flac',\n",
       " '0_22_2.flac',\n",
       " '0_23.flac',\n",
       " '0_23_0.flac',\n",
       " '0_23_1.flac',\n",
       " '0_23_2.flac',\n",
       " '0_23_3.flac',\n",
       " '0_23_4.flac',\n",
       " '0_23_5.flac',\n",
       " '0_24.flac',\n",
       " '0_24_0.flac',\n",
       " '0_24_1.flac',\n",
       " '0_24_2.flac',\n",
       " '0_24_3.flac',\n",
       " '0_24_4.flac',\n",
       " '0_25.flac',\n",
       " '0_25_0.flac',\n",
       " '0_25_1.flac',\n",
       " '0_25_2.flac',\n",
       " '0_26.flac',\n",
       " '0_26_0.flac',\n",
       " '0_26_1.flac',\n",
       " '0_26_2.flac',\n",
       " '0_26_3.flac',\n",
       " '0_26_4.flac',\n",
       " '0_26_5.flac',\n",
       " '0_26_6.flac',\n",
       " '0_27.flac',\n",
       " '0_27_0.flac',\n",
       " '0_27_1.flac',\n",
       " '0_27_2.flac',\n",
       " '0_27_3.flac',\n",
       " '0_27_4.flac',\n",
       " '0_27_5.flac',\n",
       " '0_28.flac',\n",
       " '0_28_0.flac',\n",
       " '0_28_1.flac',\n",
       " '0_28_2.flac',\n",
       " '0_29.flac',\n",
       " '0_29_0.flac',\n",
       " '0_29_1.flac',\n",
       " '0_29_2.flac',\n",
       " '0_29_3.flac',\n",
       " '0_29_4.flac',\n",
       " '0_2_0.flac',\n",
       " '0_2_1.flac',\n",
       " '0_2_2.flac',\n",
       " '0_2_3.flac',\n",
       " '0_3.flac',\n",
       " '0_30.flac',\n",
       " '0_30_0.flac',\n",
       " '0_30_1.flac',\n",
       " '0_30_2.flac',\n",
       " '0_30_3.flac',\n",
       " '0_30_4.flac',\n",
       " '0_31.flac',\n",
       " '0_31_0.flac',\n",
       " '0_31_1.flac',\n",
       " '0_31_2.flac',\n",
       " '0_32.flac',\n",
       " '0_32_0.flac',\n",
       " '0_32_1.flac',\n",
       " '0_32_2.flac',\n",
       " '0_32_3.flac',\n",
       " '0_32_4.flac',\n",
       " '0_32_5.flac',\n",
       " '0_33.flac',\n",
       " '0_33_0.flac',\n",
       " '0_33_1.flac',\n",
       " '0_33_2.flac',\n",
       " '0_33_3.flac',\n",
       " '0_33_4.flac',\n",
       " '0_33_5.flac',\n",
       " '0_34.flac',\n",
       " '0_34_0.flac',\n",
       " '0_34_1.flac',\n",
       " '0_34_2.flac',\n",
       " '0_35.flac',\n",
       " '0_35_0.flac',\n",
       " '0_35_1.flac',\n",
       " '0_35_2.flac',\n",
       " '0_35_3.flac',\n",
       " '0_35_4.flac',\n",
       " '0_36.flac',\n",
       " '0_36_0.flac',\n",
       " '0_36_1.flac',\n",
       " '0_36_2.flac',\n",
       " '0_36_3.flac',\n",
       " '0_37.flac',\n",
       " '0_37_0.flac',\n",
       " '0_37_1.flac',\n",
       " '0_37_2.flac',\n",
       " '0_37_3.flac',\n",
       " '0_38.flac',\n",
       " '0_38_0.flac',\n",
       " '0_38_1.flac',\n",
       " '0_38_2.flac',\n",
       " '0_38_3.flac',\n",
       " '0_38_4.flac',\n",
       " '0_3_0.flac',\n",
       " '0_3_1.flac',\n",
       " '0_3_2.flac',\n",
       " '0_3_3.flac',\n",
       " '0_3_4.flac',\n",
       " '0_3_5.flac',\n",
       " '0_3_6.flac',\n",
       " '0_4.flac',\n",
       " '0_4_0.flac',\n",
       " '0_4_1.flac',\n",
       " '0_4_2.flac',\n",
       " '0_5.flac',\n",
       " '0_5_0.flac',\n",
       " '0_5_1.flac',\n",
       " '0_5_2.flac',\n",
       " '0_5_3.flac',\n",
       " '0_5_4.flac',\n",
       " '0_6.flac',\n",
       " '0_6_0.flac',\n",
       " '0_6_1.flac',\n",
       " '0_6_2.flac',\n",
       " '0_6_3.flac',\n",
       " '0_7.flac',\n",
       " '0_7_0.flac',\n",
       " '0_7_1.flac',\n",
       " '0_7_2.flac',\n",
       " '0_7_3.flac',\n",
       " '0_7_4.flac',\n",
       " '0_8.flac',\n",
       " '0_8_0.flac',\n",
       " '0_8_1.flac',\n",
       " '0_8_2.flac',\n",
       " '0_8_3.flac',\n",
       " '0_8_4.flac',\n",
       " '0_8_5.flac',\n",
       " '0_8_6.flac',\n",
       " '0_9.flac',\n",
       " '0_9_0.flac',\n",
       " '0_9_1.flac',\n",
       " '0_9_2.flac',\n",
       " '0_9_3.flac',\n",
       " '0_9_4.flac',\n",
       " '0_9_5.flac',\n",
       " '0_9_6.flac',\n",
       " '10_0.flac',\n",
       " '10_0_0.flac',\n",
       " '10_0_1.flac',\n",
       " '10_0_10.flac',\n",
       " '10_0_11.flac',\n",
       " '10_0_12.flac',\n",
       " '10_0_13.flac',\n",
       " '10_0_14.flac',\n",
       " '10_0_15.flac',\n",
       " '10_0_16.flac',\n",
       " '10_0_17.flac',\n",
       " '10_0_18.flac',\n",
       " '10_0_19.flac',\n",
       " '10_0_2.flac',\n",
       " '10_0_3.flac',\n",
       " '10_0_4.flac',\n",
       " '10_0_5.flac',\n",
       " '10_0_6.flac',\n",
       " '10_0_7.flac',\n",
       " '10_0_8.flac',\n",
       " '10_0_9.flac',\n",
       " '10_1.flac',\n",
       " '10_10.flac',\n",
       " '10_10_0.flac',\n",
       " '10_10_1.flac',\n",
       " '10_10_2.flac',\n",
       " '10_10_3.flac',\n",
       " '10_10_4.flac',\n",
       " '10_10_5.flac',\n",
       " '10_10_6.flac',\n",
       " '10_10_7.flac',\n",
       " '10_10_8.flac',\n",
       " '10_10_9.flac',\n",
       " '10_11.flac',\n",
       " '10_11_0.flac',\n",
       " '10_11_1.flac',\n",
       " '10_11_2.flac',\n",
       " '10_11_3.flac',\n",
       " '10_11_4.flac',\n",
       " '10_11_5.flac',\n",
       " '10_11_6.flac',\n",
       " '10_11_7.flac',\n",
       " '10_11_8.flac',\n",
       " '10_11_9.flac',\n",
       " '10_12.flac',\n",
       " '10_12_0.flac',\n",
       " '10_12_1.flac',\n",
       " '10_12_2.flac',\n",
       " '10_12_3.flac',\n",
       " '10_12_4.flac',\n",
       " '10_12_5.flac',\n",
       " '10_12_6.flac',\n",
       " '10_12_7.flac',\n",
       " '10_12_8.flac',\n",
       " '10_13.flac',\n",
       " '10_13_0.flac',\n",
       " '10_13_1.flac',\n",
       " '10_13_2.flac',\n",
       " '10_13_3.flac',\n",
       " '10_13_4.flac',\n",
       " '10_13_5.flac',\n",
       " '10_13_6.flac',\n",
       " '10_13_7.flac',\n",
       " '10_13_8.flac',\n",
       " '10_13_9.flac',\n",
       " '10_14.flac',\n",
       " '10_14_0.flac',\n",
       " '10_14_1.flac',\n",
       " '10_14_2.flac',\n",
       " '10_14_3.flac',\n",
       " '10_14_4.flac',\n",
       " '10_14_5.flac',\n",
       " '10_14_6.flac',\n",
       " '10_14_7.flac',\n",
       " '10_14_8.flac',\n",
       " '10_15.flac',\n",
       " '10_15_0.flac',\n",
       " '10_15_1.flac',\n",
       " '10_15_2.flac',\n",
       " '10_15_3.flac',\n",
       " '10_15_4.flac',\n",
       " '10_15_5.flac',\n",
       " '10_15_6.flac',\n",
       " '10_15_7.flac',\n",
       " '10_15_8.flac',\n",
       " '10_15_9.flac',\n",
       " '10_16.flac',\n",
       " '10_16_0.flac',\n",
       " '10_16_1.flac',\n",
       " '10_16_2.flac',\n",
       " '10_16_3.flac',\n",
       " '10_16_4.flac',\n",
       " '10_16_5.flac',\n",
       " '10_16_6.flac',\n",
       " '10_16_7.flac',\n",
       " '10_16_8.flac',\n",
       " '10_16_9.flac',\n",
       " '10_17.flac',\n",
       " '10_17_0.flac',\n",
       " '10_17_1.flac',\n",
       " '10_17_2.flac',\n",
       " '10_17_3.flac',\n",
       " '10_17_4.flac',\n",
       " '10_17_5.flac',\n",
       " '10_17_6.flac',\n",
       " '10_17_7.flac',\n",
       " '10_17_8.flac',\n",
       " '10_17_9.flac',\n",
       " '10_18.flac',\n",
       " '10_18_0.flac',\n",
       " '10_18_1.flac',\n",
       " '10_18_2.flac',\n",
       " '10_18_3.flac',\n",
       " '10_18_4.flac',\n",
       " '10_18_5.flac',\n",
       " '10_18_6.flac',\n",
       " '10_18_7.flac',\n",
       " '10_18_8.flac',\n",
       " '10_18_9.flac',\n",
       " '10_19.flac',\n",
       " '10_19_0.flac',\n",
       " '10_19_1.flac',\n",
       " '10_19_2.flac',\n",
       " '10_19_3.flac',\n",
       " '10_19_4.flac',\n",
       " '10_19_5.flac',\n",
       " '10_19_6.flac',\n",
       " '10_19_7.flac',\n",
       " '10_19_8.flac',\n",
       " '10_19_9.flac',\n",
       " '10_1_0.flac',\n",
       " '10_1_1.flac',\n",
       " '10_1_10.flac',\n",
       " '10_1_11.flac',\n",
       " '10_1_12.flac',\n",
       " '10_1_13.flac',\n",
       " '10_1_14.flac',\n",
       " '10_1_15.flac',\n",
       " '10_1_16.flac',\n",
       " '10_1_17.flac',\n",
       " '10_1_18.flac',\n",
       " '10_1_19.flac',\n",
       " '10_1_2.flac',\n",
       " '10_1_3.flac',\n",
       " '10_1_4.flac',\n",
       " '10_1_5.flac',\n",
       " '10_1_6.flac',\n",
       " '10_1_7.flac',\n",
       " '10_1_8.flac',\n",
       " '10_1_9.flac',\n",
       " '10_2.flac',\n",
       " '10_20.flac',\n",
       " '10_20_0.flac',\n",
       " '10_20_1.flac',\n",
       " '10_20_2.flac',\n",
       " '10_20_3.flac',\n",
       " '10_20_4.flac',\n",
       " '10_20_5.flac',\n",
       " '10_20_6.flac',\n",
       " '10_20_7.flac',\n",
       " '10_20_8.flac',\n",
       " '10_20_9.flac',\n",
       " '10_2_0.flac',\n",
       " '10_2_1.flac',\n",
       " '10_2_10.flac',\n",
       " '10_2_11.flac',\n",
       " '10_2_12.flac',\n",
       " '10_2_13.flac',\n",
       " '10_2_14.flac',\n",
       " '10_2_15.flac',\n",
       " '10_2_16.flac',\n",
       " '10_2_17.flac',\n",
       " '10_2_18.flac',\n",
       " '10_2_19.flac',\n",
       " '10_2_2.flac',\n",
       " '10_2_3.flac',\n",
       " '10_2_4.flac',\n",
       " '10_2_5.flac',\n",
       " '10_2_6.flac',\n",
       " '10_2_7.flac',\n",
       " '10_2_8.flac',\n",
       " '10_2_9.flac',\n",
       " '10_3.flac',\n",
       " '10_3_0.flac',\n",
       " '10_3_1.flac',\n",
       " '10_3_10.flac',\n",
       " '10_3_11.flac',\n",
       " '10_3_12.flac',\n",
       " '10_3_13.flac',\n",
       " '10_3_14.flac',\n",
       " '10_3_15.flac',\n",
       " '10_3_16.flac',\n",
       " '10_3_17.flac',\n",
       " '10_3_2.flac',\n",
       " '10_3_3.flac',\n",
       " '10_3_4.flac',\n",
       " '10_3_5.flac',\n",
       " '10_3_6.flac',\n",
       " '10_3_7.flac',\n",
       " '10_3_8.flac',\n",
       " '10_3_9.flac',\n",
       " '10_4.flac',\n",
       " '10_4_0.flac',\n",
       " '10_4_1.flac',\n",
       " '10_4_10.flac',\n",
       " '10_4_11.flac',\n",
       " '10_4_12.flac',\n",
       " '10_4_13.flac',\n",
       " '10_4_14.flac',\n",
       " '10_4_15.flac',\n",
       " '10_4_16.flac',\n",
       " '10_4_17.flac',\n",
       " '10_4_18.flac',\n",
       " '10_4_19.flac',\n",
       " '10_4_2.flac',\n",
       " '10_4_3.flac',\n",
       " '10_4_4.flac',\n",
       " '10_4_5.flac',\n",
       " '10_4_6.flac',\n",
       " '10_4_7.flac',\n",
       " '10_4_8.flac',\n",
       " '10_4_9.flac',\n",
       " '10_5.flac',\n",
       " '10_5_0.flac',\n",
       " '10_5_1.flac',\n",
       " '10_5_10.flac',\n",
       " '10_5_11.flac',\n",
       " '10_5_12.flac',\n",
       " '10_5_13.flac',\n",
       " '10_5_14.flac',\n",
       " '10_5_15.flac',\n",
       " '10_5_16.flac',\n",
       " '10_5_17.flac',\n",
       " '10_5_18.flac',\n",
       " '10_5_2.flac',\n",
       " '10_5_3.flac',\n",
       " '10_5_4.flac',\n",
       " '10_5_5.flac',\n",
       " '10_5_6.flac',\n",
       " '10_5_7.flac',\n",
       " '10_5_8.flac',\n",
       " '10_5_9.flac',\n",
       " '10_6.flac',\n",
       " '10_6_0.flac',\n",
       " '10_6_1.flac',\n",
       " '10_6_10.flac',\n",
       " '10_6_11.flac',\n",
       " '10_6_12.flac',\n",
       " '10_6_13.flac',\n",
       " '10_6_14.flac',\n",
       " '10_6_2.flac',\n",
       " '10_6_3.flac',\n",
       " '10_6_4.flac',\n",
       " '10_6_5.flac',\n",
       " '10_6_6.flac',\n",
       " '10_6_7.flac',\n",
       " '10_6_8.flac',\n",
       " '10_6_9.flac',\n",
       " '10_7.flac',\n",
       " '10_7_0.flac',\n",
       " '10_7_1.flac',\n",
       " '10_7_10.flac',\n",
       " '10_7_11.flac',\n",
       " '10_7_12.flac',\n",
       " '10_7_13.flac',\n",
       " '10_7_14.flac',\n",
       " '10_7_2.flac',\n",
       " '10_7_3.flac',\n",
       " '10_7_4.flac',\n",
       " '10_7_5.flac',\n",
       " '10_7_6.flac',\n",
       " '10_7_7.flac',\n",
       " '10_7_8.flac',\n",
       " '10_7_9.flac',\n",
       " '10_8.flac',\n",
       " '10_8_0.flac',\n",
       " '10_8_1.flac',\n",
       " '10_8_10.flac',\n",
       " '10_8_11.flac',\n",
       " '10_8_12.flac',\n",
       " '10_8_13.flac',\n",
       " '10_8_2.flac',\n",
       " '10_8_3.flac',\n",
       " '10_8_4.flac',\n",
       " '10_8_5.flac',\n",
       " '10_8_6.flac',\n",
       " '10_8_7.flac',\n",
       " '10_8_8.flac',\n",
       " '10_8_9.flac',\n",
       " '10_9.flac',\n",
       " '10_9_0.flac',\n",
       " '10_9_1.flac',\n",
       " '10_9_10.flac',\n",
       " '10_9_11.flac',\n",
       " '10_9_12.flac',\n",
       " '10_9_13.flac',\n",
       " '10_9_14.flac',\n",
       " '10_9_2.flac',\n",
       " '10_9_3.flac',\n",
       " '10_9_4.flac',\n",
       " '10_9_5.flac',\n",
       " '10_9_6.flac',\n",
       " '10_9_7.flac',\n",
       " '10_9_8.flac',\n",
       " '10_9_9.flac',\n",
       " '11_0.flac',\n",
       " '11_0_0.flac',\n",
       " '11_0_1.flac',\n",
       " '11_0_10.flac',\n",
       " '11_0_11.flac',\n",
       " '11_0_12.flac',\n",
       " '11_0_13.flac',\n",
       " '11_0_14.flac',\n",
       " '11_0_2.flac',\n",
       " '11_0_3.flac',\n",
       " '11_0_4.flac',\n",
       " '11_0_5.flac',\n",
       " '11_0_6.flac',\n",
       " '11_0_7.flac',\n",
       " '11_0_8.flac',\n",
       " '11_0_9.flac',\n",
       " '11_1.flac',\n",
       " '11_10.flac',\n",
       " '11_10_0.flac',\n",
       " '11_10_1.flac',\n",
       " '11_10_2.flac',\n",
       " '11_10_3.flac',\n",
       " '11_10_4.flac',\n",
       " '11_10_5.flac',\n",
       " '11_10_6.flac',\n",
       " '11_10_7.flac',\n",
       " '11_10_8.flac',\n",
       " '11_10_9.flac',\n",
       " '11_11.flac',\n",
       " '11_11_0.flac',\n",
       " '11_11_1.flac',\n",
       " '11_11_2.flac',\n",
       " '11_11_3.flac',\n",
       " '11_11_4.flac',\n",
       " '11_11_5.flac',\n",
       " '11_11_6.flac',\n",
       " '11_11_7.flac',\n",
       " '11_11_8.flac',\n",
       " '11_12.flac',\n",
       " '11_12_0.flac',\n",
       " '11_12_1.flac',\n",
       " '11_12_2.flac',\n",
       " '11_12_3.flac',\n",
       " '11_12_4.flac',\n",
       " '11_12_5.flac',\n",
       " '11_12_6.flac',\n",
       " '11_12_7.flac',\n",
       " '11_12_8.flac',\n",
       " '11_12_9.flac',\n",
       " '11_13.flac',\n",
       " '11_13_0.flac',\n",
       " '11_13_1.flac',\n",
       " '11_13_2.flac',\n",
       " '11_13_3.flac',\n",
       " '11_13_4.flac',\n",
       " '11_13_5.flac',\n",
       " '11_13_6.flac',\n",
       " '11_13_7.flac',\n",
       " '11_13_8.flac',\n",
       " '11_13_9.flac',\n",
       " '11_15.flac',\n",
       " '11_15_0.flac',\n",
       " '11_15_1.flac',\n",
       " '11_15_2.flac',\n",
       " '11_15_3.flac',\n",
       " '11_15_4.flac',\n",
       " '11_15_5.flac',\n",
       " '11_15_6.flac',\n",
       " '11_15_7.flac',\n",
       " '11_15_8.flac',\n",
       " '11_16.flac',\n",
       " '11_16_0.flac',\n",
       " '11_16_1.flac',\n",
       " '11_16_2.flac',\n",
       " '11_16_3.flac',\n",
       " '11_16_4.flac',\n",
       " '11_16_5.flac',\n",
       " '11_16_6.flac',\n",
       " '11_16_7.flac',\n",
       " '11_16_8.flac',\n",
       " '11_16_9.flac',\n",
       " '11_17.flac',\n",
       " '11_17_0.flac',\n",
       " '11_17_1.flac',\n",
       " '11_17_2.flac',\n",
       " '11_17_3.flac',\n",
       " '11_17_4.flac',\n",
       " '11_17_5.flac',\n",
       " '11_17_6.flac',\n",
       " '11_17_7.flac',\n",
       " '11_17_8.flac',\n",
       " '11_18.flac',\n",
       " '11_18_0.flac',\n",
       " '11_18_1.flac',\n",
       " '11_18_2.flac',\n",
       " '11_18_3.flac',\n",
       " '11_18_4.flac',\n",
       " '11_18_5.flac',\n",
       " '11_18_6.flac',\n",
       " '11_18_7.flac',\n",
       " '11_18_8.flac',\n",
       " '11_18_9.flac',\n",
       " '11_19.flac',\n",
       " '11_19_0.flac',\n",
       " '11_19_1.flac',\n",
       " '11_19_2.flac',\n",
       " '11_19_3.flac',\n",
       " '11_19_4.flac',\n",
       " '11_19_5.flac',\n",
       " '11_19_6.flac',\n",
       " '11_19_7.flac',\n",
       " '11_19_8.flac',\n",
       " '11_19_9.flac',\n",
       " '11_1_0.flac',\n",
       " '11_1_1.flac',\n",
       " '11_1_10.flac',\n",
       " '11_1_11.flac',\n",
       " '11_1_12.flac',\n",
       " '11_1_13.flac',\n",
       " '11_1_14.flac',\n",
       " '11_1_2.flac',\n",
       " '11_1_3.flac',\n",
       " '11_1_4.flac',\n",
       " '11_1_5.flac',\n",
       " '11_1_6.flac',\n",
       " '11_1_7.flac',\n",
       " '11_1_8.flac',\n",
       " '11_1_9.flac',\n",
       " '11_2.flac',\n",
       " '11_20.flac',\n",
       " '11_20_0.flac',\n",
       " '11_20_1.flac',\n",
       " '11_20_2.flac',\n",
       " '11_20_3.flac',\n",
       " '11_20_4.flac',\n",
       " '11_20_5.flac',\n",
       " '11_20_6.flac',\n",
       " '11_20_7.flac',\n",
       " '11_20_8.flac',\n",
       " '11_21.flac',\n",
       " '11_21_0.flac',\n",
       " '11_21_1.flac',\n",
       " '11_21_2.flac',\n",
       " '11_21_3.flac',\n",
       " '11_21_4.flac',\n",
       " '11_21_5.flac',\n",
       " '11_21_6.flac',\n",
       " '11_21_7.flac',\n",
       " '11_21_8.flac',\n",
       " '11_21_9.flac',\n",
       " '11_22.flac',\n",
       " '11_22_0.flac',\n",
       " '11_22_1.flac',\n",
       " '11_22_2.flac',\n",
       " '11_22_3.flac',\n",
       " '11_22_4.flac',\n",
       " '11_22_5.flac',\n",
       " '11_22_6.flac',\n",
       " '11_22_7.flac',\n",
       " '11_22_8.flac',\n",
       " '11_22_9.flac',\n",
       " '11_23.flac',\n",
       " '11_23_0.flac',\n",
       " '11_23_1.flac',\n",
       " '11_23_2.flac',\n",
       " '11_23_3.flac',\n",
       " '11_23_4.flac',\n",
       " '11_23_5.flac',\n",
       " '11_23_6.flac',\n",
       " '11_23_7.flac',\n",
       " '11_23_8.flac',\n",
       " '11_25.flac',\n",
       " '11_25_0.flac',\n",
       " '11_25_1.flac',\n",
       " '11_25_2.flac',\n",
       " '11_25_3.flac',\n",
       " '11_25_4.flac',\n",
       " '11_25_5.flac',\n",
       " '11_25_6.flac',\n",
       " '11_25_7.flac',\n",
       " '11_25_8.flac',\n",
       " '11_25_9.flac',\n",
       " '11_26.flac',\n",
       " '11_26_0.flac',\n",
       " '11_26_1.flac',\n",
       " '11_26_2.flac',\n",
       " '11_26_3.flac',\n",
       " '11_26_4.flac',\n",
       " '11_26_5.flac',\n",
       " '11_26_6.flac',\n",
       " '11_26_7.flac',\n",
       " '11_26_8.flac',\n",
       " '11_26_9.flac',\n",
       " '11_27.flac',\n",
       " '11_27_0.flac',\n",
       " '11_27_1.flac',\n",
       " '11_27_2.flac',\n",
       " '11_27_3.flac',\n",
       " '11_27_4.flac',\n",
       " '11_27_5.flac',\n",
       " '11_27_6.flac',\n",
       " '11_27_7.flac',\n",
       " '11_27_8.flac',\n",
       " '11_27_9.flac',\n",
       " '11_28.flac',\n",
       " '11_28_0.flac',\n",
       " '11_28_1.flac',\n",
       " '11_28_2.flac',\n",
       " '11_28_3.flac',\n",
       " '11_28_4.flac',\n",
       " '11_28_5.flac',\n",
       " '11_28_6.flac',\n",
       " '11_28_7.flac',\n",
       " '11_28_8.flac',\n",
       " '11_28_9.flac',\n",
       " '11_2_0.flac',\n",
       " '11_2_1.flac',\n",
       " '11_2_10.flac',\n",
       " '11_2_11.flac',\n",
       " '11_2_12.flac',\n",
       " '11_2_13.flac',\n",
       " '11_2_14.flac',\n",
       " '11_2_2.flac',\n",
       " '11_2_3.flac',\n",
       " '11_2_4.flac',\n",
       " '11_2_5.flac',\n",
       " '11_2_6.flac',\n",
       " '11_2_7.flac',\n",
       " '11_2_8.flac',\n",
       " '11_2_9.flac',\n",
       " '11_3.flac',\n",
       " '11_3_0.flac',\n",
       " '11_3_1.flac',\n",
       " '11_3_10.flac',\n",
       " '11_3_11.flac',\n",
       " '11_3_12.flac',\n",
       " '11_3_13.flac',\n",
       " '11_3_2.flac',\n",
       " '11_3_3.flac',\n",
       " '11_3_4.flac',\n",
       " '11_3_5.flac',\n",
       " '11_3_6.flac',\n",
       " '11_3_7.flac',\n",
       " '11_3_8.flac',\n",
       " '11_3_9.flac',\n",
       " '11_4.flac',\n",
       " '11_4_0.flac',\n",
       " '11_4_1.flac',\n",
       " '11_4_10.flac',\n",
       " '11_4_11.flac',\n",
       " '11_4_12.flac',\n",
       " '11_4_2.flac',\n",
       " '11_4_3.flac',\n",
       " '11_4_4.flac',\n",
       " '11_4_5.flac',\n",
       " '11_4_6.flac',\n",
       " '11_4_7.flac',\n",
       " '11_4_8.flac',\n",
       " '11_4_9.flac',\n",
       " '11_5.flac',\n",
       " '11_5_0.flac',\n",
       " '11_5_1.flac',\n",
       " '11_5_10.flac',\n",
       " '11_5_11.flac',\n",
       " '11_5_12.flac',\n",
       " '11_5_13.flac',\n",
       " '11_5_14.flac',\n",
       " '11_5_2.flac',\n",
       " '11_5_3.flac',\n",
       " '11_5_4.flac',\n",
       " '11_5_5.flac',\n",
       " '11_5_6.flac',\n",
       " '11_5_7.flac',\n",
       " '11_5_8.flac',\n",
       " '11_5_9.flac',\n",
       " '11_6.flac',\n",
       " '11_6_0.flac',\n",
       " '11_6_1.flac',\n",
       " '11_6_2.flac',\n",
       " '11_6_3.flac',\n",
       " '11_6_4.flac',\n",
       " '11_6_5.flac',\n",
       " '11_6_6.flac',\n",
       " '11_6_7.flac',\n",
       " '11_6_8.flac',\n",
       " '11_7.flac',\n",
       " '11_7_0.flac',\n",
       " '11_7_1.flac',\n",
       " '11_7_2.flac',\n",
       " '11_7_3.flac',\n",
       " '11_7_4.flac',\n",
       " '11_7_5.flac',\n",
       " '11_7_6.flac',\n",
       " '11_7_7.flac',\n",
       " '11_7_8.flac',\n",
       " '11_8.flac',\n",
       " '11_8_0.flac',\n",
       " '11_8_1.flac',\n",
       " '11_8_2.flac',\n",
       " '11_8_3.flac',\n",
       " '11_8_4.flac',\n",
       " '11_8_5.flac',\n",
       " '11_8_6.flac',\n",
       " '11_8_7.flac',\n",
       " '11_8_8.flac',\n",
       " '11_9.flac',\n",
       " '11_9_0.flac',\n",
       " '11_9_1.flac',\n",
       " '11_9_2.flac',\n",
       " '11_9_3.flac',\n",
       " '11_9_4.flac',\n",
       " '11_9_5.flac',\n",
       " '11_9_6.flac',\n",
       " '11_9_7.flac',\n",
       " '11_9_8.flac',\n",
       " '11_9_9.flac',\n",
       " '12_0.flac',\n",
       " '12_0_0.flac',\n",
       " '12_0_1.flac',\n",
       " '12_0_2.flac',\n",
       " '12_0_3.flac',\n",
       " '12_0_4.flac',\n",
       " '12_0_5.flac',\n",
       " '12_0_6.flac',\n",
       " '12_0_7.flac',\n",
       " '12_0_8.flac',\n",
       " '12_0_9.flac',\n",
       " '12_1.flac',\n",
       " '12_10.flac',\n",
       " '12_10_0.flac',\n",
       " '12_10_1.flac',\n",
       " '12_10_2.flac',\n",
       " '12_10_3.flac',\n",
       " '12_10_4.flac',\n",
       " '12_10_5.flac',\n",
       " '12_10_6.flac',\n",
       " '12_10_7.flac',\n",
       " '12_10_8.flac',\n",
       " '12_10_9.flac',\n",
       " '12_11.flac',\n",
       " '12_11_0.flac',\n",
       " '12_11_1.flac',\n",
       " '12_11_2.flac',\n",
       " '12_11_3.flac',\n",
       " '12_11_4.flac',\n",
       " '12_11_5.flac',\n",
       " '12_11_6.flac',\n",
       " '12_11_7.flac',\n",
       " '12_11_8.flac',\n",
       " '12_12.flac',\n",
       " '12_12_0.flac',\n",
       " '12_12_1.flac',\n",
       " '12_12_2.flac',\n",
       " '12_12_3.flac',\n",
       " '12_12_4.flac',\n",
       " '12_12_5.flac',\n",
       " '12_12_6.flac',\n",
       " '12_12_7.flac',\n",
       " '12_12_8.flac',\n",
       " '12_14.flac',\n",
       " '12_14_0.flac',\n",
       " '12_14_1.flac',\n",
       " '12_14_2.flac',\n",
       " '12_14_3.flac',\n",
       " '12_14_4.flac',\n",
       " '12_14_5.flac',\n",
       " '12_14_6.flac',\n",
       " '12_14_7.flac',\n",
       " '12_14_8.flac',\n",
       " '12_14_9.flac',\n",
       " '12_15.flac',\n",
       " '12_15_0.flac',\n",
       " '12_15_1.flac',\n",
       " '12_15_2.flac',\n",
       " '12_15_3.flac',\n",
       " '12_15_4.flac',\n",
       " '12_15_5.flac',\n",
       " '12_15_6.flac',\n",
       " '12_15_7.flac',\n",
       " '12_15_8.flac',\n",
       " '12_15_9.flac',\n",
       " '12_16.flac',\n",
       " '12_16_0.flac',\n",
       " '12_16_1.flac',\n",
       " '12_16_2.flac',\n",
       " '12_16_3.flac',\n",
       " '12_16_4.flac',\n",
       " '12_16_5.flac',\n",
       " '12_16_6.flac',\n",
       " '12_16_7.flac',\n",
       " '12_16_8.flac',\n",
       " '12_16_9.flac',\n",
       " '12_18.flac',\n",
       " '12_18_0.flac',\n",
       " '12_18_1.flac',\n",
       " '12_18_2.flac',\n",
       " '12_18_3.flac',\n",
       " '12_18_4.flac',\n",
       " '12_18_5.flac',\n",
       " '12_18_6.flac',\n",
       " '12_18_7.flac',\n",
       " '12_18_8.flac',\n",
       " '12_18_9.flac',\n",
       " '12_1_0.flac',\n",
       " '12_1_1.flac',\n",
       " '12_1_2.flac',\n",
       " '12_1_3.flac',\n",
       " '12_1_4.flac',\n",
       " '12_1_5.flac',\n",
       " '12_1_6.flac',\n",
       " '12_1_7.flac',\n",
       " '12_1_8.flac',\n",
       " '12_1_9.flac',\n",
       " '12_2.flac',\n",
       " '12_22.flac',\n",
       " '12_22_0.flac',\n",
       " '12_22_1.flac',\n",
       " '12_22_2.flac',\n",
       " '12_22_3.flac',\n",
       " '12_22_4.flac',\n",
       " '12_22_5.flac',\n",
       " '12_22_6.flac',\n",
       " '12_22_7.flac',\n",
       " '12_22_8.flac',\n",
       " '12_22_9.flac',\n",
       " '12_23.flac',\n",
       " '12_23_0.flac',\n",
       " '12_23_1.flac',\n",
       " '12_23_2.flac',\n",
       " '12_23_3.flac',\n",
       " '12_23_4.flac',\n",
       " '12_23_5.flac',\n",
       " '12_23_6.flac',\n",
       " '12_23_7.flac',\n",
       " '12_23_8.flac',\n",
       " '12_23_9.flac',\n",
       " '12_24.flac',\n",
       " '12_24_0.flac',\n",
       " '12_24_1.flac',\n",
       " '12_24_2.flac',\n",
       " '12_24_3.flac',\n",
       " '12_24_4.flac',\n",
       " '12_24_5.flac',\n",
       " '12_24_6.flac',\n",
       " '12_24_7.flac',\n",
       " '12_24_8.flac',\n",
       " '12_24_9.flac',\n",
       " '12_25.flac',\n",
       " '12_25_0.flac',\n",
       " '12_25_1.flac',\n",
       " '12_25_2.flac',\n",
       " '12_25_3.flac',\n",
       " '12_25_4.flac',\n",
       " '12_25_5.flac',\n",
       " '12_25_6.flac',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WANDB_NAME = 'jefe-jeff' # Fill in your Weights & Biases ID here.\n",
    "\n",
    "def run(system, config, ckpt_dir, epochs=1, monitor_key='val_loss', \n",
    "        use_gpu=False, seed=1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    SystemClass = globals()[system]\n",
    "    system = SystemClass(**config)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(MODEL_PATH, ckpt_dir),\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=monitor_key, \n",
    "    mode='min')\n",
    "\n",
    "    wandb.init(project='cs224s', entity=WANDB_NAME, name=ckpt_dir, \n",
    "         config=config, sync_tensorboard=True)\n",
    "    wandb_logger = WandbLogger()\n",
    "\n",
    "    if use_gpu:\n",
    "        trainer = pl.Trainer(\n",
    "        gpus=1, max_epochs=epochs, min_epochs=epochs,\n",
    "        checkpoint_callback=checkpoint_callback, logger=wandb_logger)\n",
    "    else:\n",
    "        trainer = pl.Trainer(\n",
    "        max_epochs=epochs, min_epochs=epochs,\n",
    "        checkpoint_callback=checkpoint_callback, logger=wandb_logger)\n",
    "\n",
    "    trainer.fit(system)\n",
    "    result = trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "demographic-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'learning_rate': 1e-5, \n",
    "    'batch_size': 16, \n",
    "    'weight_decay': 0, \n",
    "}\n",
    "\n",
    "run(system=\"LightningTextBERTQA\", config=config, ckpt_dir='textbert_0gen', epochs=5, \n",
    "    use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-compromise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu100.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu100:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
